mpidag(1)
=========
:doctype: manpage


Name
----
mpidag - a tool for running computational workflows expressed as DAGs
(Directed Acyclic Graphs) on computational clusters using MPI.


Synopsis
--------
*mpidag* [*-h*] [*-v*] [*-q*] [*-s*] [*-L* 'path'] [*-o* 'path'] [*-e* 'path'] 'workflow.dag'


Description
-----------
*mpidag* is a tool used to run HTC (High Throughput Computing) scientific
workflows on systems designed for HPC (High Performance Computing).
Many HPC systems have custom architectures that are optimized for
tightly-coupled, parallel applications. These systems commonly have
exotic, low-latency networks that are designed for passing short 
messages very quickly between compute nodes. Many of these networks are so
highly optimized that the compute nodes do not even support a TCP/IP
stack. This makes it impossible to run HTC applications using software
that was designed for commodity clusters, such as Condor.

*mpidag* was developed to enable loosely-coupled HTC applications such as
scientific workflows to take advantage of HPC systems. In order to get
around the network issues outlined above, *mpidag* uses MPI (Message
Passing Interface), a commonly used API for writing SPMD (Single
Process, Multiple Data) parallel applications. Most HPC systems have an
MPI implementation that works on whatever exotic network architecture
the system uses.

An *mpidag* job consists of a single master process (this process is rank
0 in MPI parlance) and several worker processes. The master process
manages the workflow and assigns workflow tasks to workers for execution. 
The workers execute the tasks and return the results to the master. Any 
output written to stdout or stderr by the tasks is captured (see 
<<TASK_STDIO,*TASK STDIO*>>).

*mpidag* applications are expressed as DAGs (Directed Acyclic Graphs)
(see <<DAG_FILES,*DAG FILES*>>). Each node in the graph represents a task, 
and the edges represent dependencies between the tasks that constrain the 
order in which the tasks are executed. Each task is a program and a set of
parameters that need to be run (i.e. a command and some optional 
arguments). The dependencies typically represent data flow dependencies in
the application, where the output files produced by one task are needed
as inputs for another.

If an error occurs while executing a DAG that causes the workflow to
stop, it can be restarted using a rescue file, which records the
progress of the workflow (see <<RESCUE_FILES,*RESCUE FILES*>>). This 
enables *mpidag* to pick up running the workflow where it stopped.

*mpidag* allows applications expressed as a DAG to be executed in 
parallel on a large number of compute nodes. It is designed to be simple,
lightweight and robust.


Options
-------
*-h*::
*--help*::
    Print help message

*-v*::
*--verbose*::
    Increase logging verbosity. Adding multiple *-v* increases the
    level more. The default log level is 'INFO'. (see <<LOGGING,
    *LOGGING*>>)

*-q*::
*--quiet*::
    Decrease logging verbosity. Adding multiple *-q* decreases the
    level more. The default log level is 'INFO'. (see <<LOGGING,
    *LOGGING*>>)

*-s*::
*--skip-rescue*::
    Ignore the rescue file for 'workflow.dag' if it exists. Note that
    *mpidag* will still create a new rescue file for the current run.
    The default behavior is to use the rescue file if one is found.
    (see <<RESCUE_FILES,*RESCUE FILES*>>)

*-L* 'path'::
*--logfile* 'path'::
    Path to file for logging messages. By default, error-type log messages
    go to stderr, and info-type log messages go to stdout. (see <<LOGGING,
    *LOGGING*>>)

*-o* 'path'::
*--stdout* 'path'::
    Path to file for task stdout. *mpidag* will append '.XXX' to path
    where 'XXX' is the retry number according to the retry numbering
    scheme. The default is to use 'PATH.out.XXX' where 'PATH' is the 
    path to the input DAG file. (see <<TASK_STDIO,*TASK STDIO*>>)

*-e* 'path'::
*--stderr* 'path'::
    Path to file for task stderr. *mpidag* will append '.XXX' to path
    where 'XXX' is the retry number according to the retry numbering
    scheme. The default is to use 'PATH.err.XXX' where 'PATH' is the
    path to the input DAG file. (see <<TASK_STDIO,*TASK STDIO*>>)

*-m* 'N'::
*--max-failures* 'N'::
    Stop submitting new tasks after 'N' tasks have failed. Once 'N' 
    has been reached, *mpidag* will finish running any tasks that have
    been started, but will not start any more tasks. This option is
    used to prevent *mpidag* from continuing to run a workflow that 
    is suffering from a systematic error, such as a missing binary or
    an invalid path. The default is 0, which means unlimited failures
    are allowed.


[[DAG_FILES]]
DAG Files
---------
*mpidag* workflows are expressed using a simple text-based format similar
to that used by Condor DAGMan. There are only two record types allowed
in a DAG file: *TASK* and *EDGE*. Any blank lines in the DAG (lines with
all whitespace characters) are ignored, as are any lines beginning with
# (note that # can only appear at the beginning of a line, not in the
middle).

The format of a *TASK* record is:
[verse]
    *TASK* 'id' 'executable' ['arguments...']

Where 'id' is the ID of the task, 'executable' is the path to the executable
or script to run, and 'arguments...' is a space-separated list of
arguments to pass to the task. An example is:
------------
    TASK t01 /bin/program -a -b
------------
The format of an *EDGE* record is:
[verse]
    *EDGE* 'parent' 'child'

Where 'parent' is the ID of the parent task, and 'child' is the ID of the
child task. An example *EDGE* record is:
------------
    EDGE t01 t02
------------
A simple diamond-shaped workflow would look like this:
------------
    # diamond.dag
    TASK A /bin/echo "I am A"
    TASK B /bin/echo "I am B"
    TASK C /bin/echo "I am C"
    TASK D /bin/echo "I am D"

    EDGE A B
    EDGE A C
    EDGE B D
    EDGE C D
------------


[[RESCUE_FILES]]
Rescue Files
------------
Many different types of errors can occur when running a DAG. One or
more of the tasks may fail, the MPI job may run out of wall time,
*mpidag* may segfault (we hope not), the system may crash, etc. In order
to ensure that the DAG does not need to be restarted from the beginning
after an error, *mpidag* generates a rescue file for each workflow.

The rescue file is a simple text file that lists all of the tasks in
the workflow that have finished successfully. This file is updated each
time a task finishes, and is flushed periodically so that if the work-
flow fails and the user restarts it, *mpidag* can determine which tasks
still need to be executed. As such, the rescue file is a sort-of trans-
action log for the workflow.

The rescue file contains zero or more DONE records. The format of these
records is:
[verse]
    *DONE* 'taskid'

Where 'taskid' is the ID of the task that finished successfully.

Rescue files are named 'DAGNAME.rescue.XXX' where 'DAGNAME' is the path to
the input DAG file, and XXX is the current retry sequence number for
the rescue file (see <<RETRY_NUMBERING,*RETRY NUMBERING*>>).


[[LOGGING]]
Logging
-------
By default, all logging messages are printed to stdout. If you turn up
the logging using *-v* then you may end up with a lot of stdout being
forwarded from the workers to the master. To avoid that you can specify
a log file using the *-L* argument. Note that if you use *-L* you will get
N log files where N is the number of processes (master and workers),
and that these N log files will not be merged into one at the end. Each
process will append its rank to the log file name, so if you use:
------------
    -L log/foo.log
------------
you will get 'log/foo.log.0', 'log/foo.log.1', and so on.

The log levels in order of severity are: FATAL, ERROR, WARN, INFO, DEBUG, 
and TRACE.

The default logging level is INFO. The logging levels can be
increased/decreased with *-v* and *-q*.


[[TASK_STDIO]]
Task STDIO
----------
By default the stdout/stderr of tasks will be saved into files called
'DAGNAME.out.XXX' and 'DAGNAME.err.XXX', where 'DAGNAME' is the path to the
input DAG file, and 'XXX' is the current retry sequence number for the 
out/err file  (see <<RETRY_NUMBERING,*RETRY NUMBERING*>>). You can change 
the path of these files with the *-o* and *-e* arguments. Note that the 
stdio of all workers will be merged into one out and one err file by the 
master at the end, so I/O from different workers will appear in a random 
order, but I/O from each worker will appear in the order that it was 
generated. Also note that, if the job fails for any reason, the outputs 
will not be merged, but instead there will be one file for each worker 
named as above, but with '.X' appended where 'X' is the worker's rank.


[[RETRY_NUMBERING]]
Retry Numbering
---------------
Each of the output, error, and rescue files are numbered with a suffix
that starts at '.000' and increments by 1 each time the workflow is run
(e.g. '.000', '.001', '.002', and so on). This is done so that the user and
*mpidag* can distinguish between the outputs of different workflow runs.


Known Issues
------------
fork() and exec()
~~~~~~~~~~~~~~~~~
In order for the worker processes to start tasks on the compute
node the compute nodes must support the *fork()* and *exec()* system
calls. If your target machine runs a stripped-down OS on the
compute nodes that does not support these system calls, then
*mpidag* will not work.

CPU Usage
~~~~~~~~~
Many MPI implementations are optimized so that message sends and
receives do not block. The reasoning is that blocking adds over-
head and, since many HPC systems use space sharing on dedicated
hardware, there are no other processes competing, so spinning
instead of blocking can produce better performance. On those
MPI implementations the master and worker processes will run at
100% CPU usage even when they are waiting. If this is a problem
on your system, then there are some MPI implementations that
'do' block on message send and receive. To test *mpidag*, for
example, we use MPICH2 with the ch3:sock device instead of the
ch3:nemesis device to avoid this issue.

Author
------
Gideon Juve `<juve@usc.edu>`
